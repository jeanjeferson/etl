# ETL Pipeline - ExtraÃ§Ã£o SQL e Upload FTP

Sistema ETL para extraÃ§Ã£o de dados de SQL Server e upload automÃ¡tico para FTP/SFTP.

## ğŸ“‹ Requisitos

- Python >= 3.12
- ODBC Driver for SQL Server instalado
- Acesso ao banco de dados SQL Server
- Acesso ao servidor FTP/SFTP

## ğŸš€ InstalaÃ§Ã£o

### OpÃ§Ã£o 1: Docker (Recomendado)

#### 1. Configurar credenciais

Copie o arquivo `env.example` para `.env` e preencha com suas credenciais:

```bash
# Linux/Mac
cp env.example .env

# Windows
copy env.example .env
```

Edite o arquivo `.env`:

```env
DB_DRIVER=ODBC Driver 18 for SQL Server
DB_SERVER=seu-servidor.database.windows.net
DB_PORT=1433
DB_UID=seu_usuario
DB_PWD=sua_senha
```

#### 2. Build e executar com Docker Compose

```bash
# Build da imagem
docker-compose build

# Iniciar o serviÃ§o
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar o serviÃ§o
docker-compose down
```

A API estarÃ¡ disponÃ­vel em: http://localhost:8000

#### 3. Build e executar com Docker (sem compose)

```bash
# Build da imagem
docker build -t etl-pipeline-api .

# Executar container
docker run -d \
  --name etl-api \
  -p 8000:8000 \
  --env-file .env \
  -v $(pwd)/data:/app/data \
  etl-pipeline-api

# Ver logs
docker logs -f etl-api

# Parar container
docker stop etl-api
docker rm etl-api
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Local

#### 1. Criar ambiente virtual e instalar dependÃªncias

```bash
# Criar ambiente virtual
uv venv

# Ativar ambiente (Windows)
.venv\Scripts\activate

# Instalar dependÃªncias
uv pip install -e .
```

#### 3. Verificar driver ODBC instalado

Para verificar quais drivers ODBC estÃ£o instalados no Windows:

```powershell
Get-OdbcDriver | Select-Object -Property Name
```

Drivers comuns:
- `ODBC Driver 18 for SQL Server` (mais recente)
- `ODBC Driver 17 for SQL Server`
- `SQL Server Native Client 11.0`

## ğŸ“ Estrutura do Projeto

```
etl/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ databases.yaml          # Lista de databases a processar
â”œâ”€â”€ sql/                         # Arquivos SQL com queries
â”‚   â”œâ”€â”€ clientes.sql
â”‚   â”œâ”€â”€ consultor.sql
â”‚   â”œâ”€â”€ estoque.sql
â”‚   â”œâ”€â”€ lojas.sql
â”‚   â”œâ”€â”€ metas_emp.sql
â”‚   â”œâ”€â”€ meta_fun.sql
â”‚   â”œâ”€â”€ produtos.sql
â”‚   â””â”€â”€ vendas.sql
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ sql_query.py            # Classe para executar queries
â”‚   â””â”€â”€ ftp_uploader.py         # Classe para upload FTP
â”œâ”€â”€ data/                        # SaÃ­da dos arquivos parquet (gerado)
â”œâ”€â”€ api.py                       # FastAPI application (execuÃ§Ã£o assÃ­ncrona)
â”œâ”€â”€ run_sql.py                   # Script principal (execuÃ§Ã£o direta)
â”œâ”€â”€ pyproject.toml              # DependÃªncias do projeto
â”œâ”€â”€ Dockerfile                   # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ .dockerignore               # Arquivos excluÃ­dos do Docker build
â”œâ”€â”€ .gitignore                  # Arquivos excluÃ­dos do Git
â”œâ”€â”€ env.example                 # Exemplo de variÃ¡veis de ambiente
â””â”€â”€ .env                         # Credenciais (criar a partir do env.example)
```

## ğŸ¯ Uso

### OpÃ§Ã£o 1: FastAPI (Recomendado para execuÃ§Ã£o assÃ­ncrona)

Inicie o servidor da API:

```bash
uvicorn api:app --reload
```

A API estarÃ¡ disponÃ­vel em `http://localhost:8000`

**Endpoints disponÃ­veis:**

- `GET /` - InformaÃ§Ãµes da API
- `GET /health` - Health check
- `POST /run-pipeline` - Inicia pipeline ETL em background
- `GET /jobs/{job_id}` - Consulta status de um job especÃ­fico
- `GET /jobs` - Lista todos os jobs
- `GET /docs` - DocumentaÃ§Ã£o interativa (Swagger UI)

**Exemplo de uso:**

```bash
# Iniciar pipeline
curl -X POST "http://localhost:8000/run-pipeline?output_dir=data&forecast_type=data&verbose=true"
# Retorna: {"job_id": "uuid", "status": "pending", ...}

# Consultar status
curl "http://localhost:8000/jobs/{job_id}"

# Listar todos os jobs
curl "http://localhost:8000/jobs"
```

### OpÃ§Ã£o 2: Script direto (ExecuÃ§Ã£o sÃ­ncrona)

```bash
python run_sql.py
```

Isso irÃ¡:
1. ğŸ“Š Executar todas as queries SQL dos arquivos em `sql/`
2. ğŸ’¾ Salvar resultados em `data/{database}/*.parquet`
3. ğŸ“¤ Fazer upload automÃ¡tico para FTP em `ai/{database}/data/`
4. ğŸ“‹ Exibir resumo completo da execuÃ§Ã£o

### Executar apenas extraÃ§Ã£o SQL

```python
from utils.sql_query import SQLQuery

extractor = SQLQuery()
extractor.verbose = True
results = extractor.execute_all_queries(output_base_dir="data")
```

### Executar apenas upload FTP

```python
from utils.ftp_uploader import ForecastFTPUploader
from pathlib import Path

ftp = ForecastFTPUploader()
if ftp._connect():
    db_path = Path('data/005ATS_ERP_BI')
    parquet_files = [str(f) for f in db_path.glob('*.parquet')]
    
    result = ftp.upload_data(
        database_name='005ATS_ERP_BI',
        forecast_type='data',
        file_paths=parquet_files
    )
    ftp.disconnect()
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Databases (`config/databases.yaml`)

Lista os databases que serÃ£o processados:

```yaml
databases:
  - '005ATS_ERP_BI'
  - '005NO_ERP_BI'
  - '005RG_ERP_BI'
  # ...
```

### Queries SQL (`sql/*.sql`)

Adicione arquivos `.sql` na pasta `sql/`. Cada arquivo serÃ¡:
- Executado em todos os databases configurados
- Salvo como `data/{database}/{nome_arquivo}.parquet`
- Enviado para `ai/{database}/data/{nome_arquivo}.parquet` no FTP

### Credenciais FTP

As credenciais FTP estÃ£o hardcoded em `utils/ftp_uploader.py`. 
Para ambientes de produÃ§Ã£o, considere movÃª-las para variÃ¡veis de ambiente.

## ğŸ“Š SaÃ­da

### Estrutura de arquivos locais

```
data/
â”œâ”€â”€ 005ATS_ERP_BI/
â”‚   â”œâ”€â”€ clientes.parquet
â”‚   â”œâ”€â”€ consultor.parquet
â”‚   â”œâ”€â”€ estoque.parquet
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 005NO_ERP_BI/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

### Estrutura no FTP

```
ai/
â”œâ”€â”€ 005ATS_ERP_BI/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ clientes.parquet
â”‚       â”œâ”€â”€ consultor.parquet
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
```

## ğŸ”§ Troubleshooting

### Erro: "Nome da fonte de dados nÃ£o encontrado"

```
pyodbc.InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] 
Nome da fonte de dados nÃ£o encontrado...')
```

**SoluÃ§Ãµes:**
1. Verifique se o arquivo `.env` existe e estÃ¡ configurado
2. Verifique se o driver ODBC especificado estÃ¡ instalado
3. Teste a conexÃ£o manualmente

### Erro: "FTP connection failed"

**SoluÃ§Ãµes:**
1. Verifique se o servidor FTP estÃ¡ acessÃ­vel
2. Verifique credenciais em `utils/ftp_uploader.py`
3. Verifique firewall/rede

### Arquivos parquet vazios

Se as queries retornam 0 linhas:
1. Verifique se as tabelas existem no database
2. Verifique permissÃµes do usuÃ¡rio do banco de dados
3. Teste as queries manualmente

## ğŸ“¦ DependÃªncias Principais

- **pandas**: ManipulaÃ§Ã£o de dados
- **sqlalchemy**: ConexÃ£o com SQL Server
- **pyodbc**: Driver ODBC
- **pyarrow**: Suporte a arquivos Parquet
- **pyyaml**: Leitura de configs YAML
- **python-dotenv**: VariÃ¡veis de ambiente
- **paramiko**: ConexÃ£o FTP/SFTP
- **fastapi**: Framework web para APIs
- **uvicorn**: Servidor ASGI para FastAPI

## ğŸ³ Docker

### Arquivos Docker

- **Dockerfile**: Imagem base com Python 3.12 e ODBC Driver 18
- **docker-compose.yml**: OrquestraÃ§Ã£o com volumes e variÃ¡veis de ambiente
- **.dockerignore**: Arquivos excluÃ­dos do build

### Volumes

O docker-compose.yml configura os seguintes volumes:

- `./data:/app/data` - Dados parquet gerados
- `./logs:/app/logs` - Logs da aplicaÃ§Ã£o
- `./config:/app/config` - ConfiguraÃ§Ãµes (read-only)
- `./sql:/app/sql` - Queries SQL (read-only)

### VariÃ¡veis de Ambiente

Configure no arquivo `.env`:

```env
DB_DRIVER=ODBC Driver 18 for SQL Server
DB_SERVER=seu-servidor.database.windows.net
DB_PORT=1433
DB_UID=seu_usuario
DB_PWD=sua_senha
TZ=America/Sao_Paulo
```

### Health Check

O container inclui health check automÃ¡tico:
- Intervalo: 30s
- Timeout: 10s
- Retries: 3
- Start period: 40s
- Endpoint: `http://localhost:8000/health`

## ğŸ“ LicenÃ§a

Este projeto Ã© de uso interno.

