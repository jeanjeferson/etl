"""
ETL Pipeline Runner
Executa queries SQL e faz upload dos resultados para FTP/SFTP
"""

from utils.sql_query import SQLQuery
from utils.ftp_uploader import ForecastFTPUploader
from utils.upload_supabase import SupabaseUploader
from pathlib import Path
from typing import Dict, Any, List, Optional
import time
import tempfile
import shutil


def run_etl_pipeline(output_dir: str = "data", verbose: bool = True) -> Dict[str, Any]:
    """
    Executa pipeline de extra√ß√£o de dados SQL.
    
    Args:
        output_dir: Diret√≥rio base para salvar os arquivos parquet
        verbose: Exibir logs detalhados
        
    Returns:
        Dicion√°rio com estat√≠sticas de execu√ß√£o
    """
    print("=" * 80)
    print("üìä FASE 1: EXTRA√á√ÉO DE DADOS SQL")
    print("=" * 80)
    
    try:
        # Instanciar SQLQuery
        extractor = SQLQuery()
        extractor.verbose = verbose
        
        # Executar todas as queries
        results = extractor.execute_all_queries(output_base_dir=output_dir)
        
        return results
        
    except Exception as e:
        print(f"\n‚ùå Erro na extra√ß√£o de dados: {e}")
        return {
            'success': False,
            'error': str(e),
            'successful': 0,
            'failed': 0,
            'total_executions': 0
        }


def upload_to_ftp(data_dir: str = "data", forecast_type: str = "data") -> Dict[str, Any]:
    """
    Faz upload dos arquivos parquet gerados para FTP/SFTP.
    
    Args:
        data_dir: Diret√≥rio base contendo as pastas de databases
        forecast_type: Tipo de dados ('data', 'vendas', 'volume', etc)
        
    Returns:
        Dicion√°rio com estat√≠sticas de upload
    """
    print("\n" + "=" * 80)
    print("üì§ FASE 2: UPLOAD PARA FTP/SFTP")
    print("=" * 80)
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"‚ùå Diret√≥rio {data_dir} n√£o encontrado")
        return {
            'success': False,
            'error': f'Directory {data_dir} not found',
            'total_uploads': 0,
            'successful_uploads': 0,
            'failed_uploads': 0
        }
    
    # Estat√≠sticas
    upload_stats = {
        'total_uploads': 0,
        'successful_uploads': 0,
        'failed_uploads': 0,
        'databases_processed': [],
        'errors': []
    }
    
    try:
        # Criar conex√£o FTP
        ftp = ForecastFTPUploader()
        
        if not ftp._connect():
            print("‚ùå Falha ao conectar no servidor FTP")
            return {
                'success': False,
                'error': 'FTP connection failed',
                **upload_stats
            }
        
        print("‚úÖ Conectado ao FTP com sucesso\n")
        
        # Iterar sobre cada database
        database_folders = [d for d in data_path.iterdir() if d.is_dir()]
        
        if not database_folders:
            print(f"‚ö†Ô∏è  Nenhuma pasta de database encontrada em {data_dir}")
            ftp.disconnect()
            return {
                'success': False,
                'error': 'No database folders found',
                **upload_stats
            }
        
        print(f"üìÅ Encontradas {len(database_folders)} pastas de databases\n")
        
        for db_folder in database_folders:
            database_name = db_folder.name
            print(f"üìä Processando database: {database_name}")
            
            # Coletar arquivos parquet
            parquet_files = list(db_folder.glob('*.parquet'))
            
            if not parquet_files:
                print(f"   ‚ö†Ô∏è  Nenhum arquivo parquet encontrado em {db_folder}")
                continue
            
            print(f"   üìÑ {len(parquet_files)} arquivos encontrados")
            
            # Converter para lista de strings (caminhos completos)
            file_paths = [str(f) for f in parquet_files]
            
            try:
                # Upload para FTP
                result = ftp.upload_data(
                    database_name=database_name,
                    forecast_type=forecast_type,
                    file_paths=file_paths
                )
                
                upload_stats['total_uploads'] += len(parquet_files)
                
                if result['success']:
                    upload_stats['successful_uploads'] += len(result['uploaded_files'])
                    upload_stats['databases_processed'].append(database_name)
                    print(f"   ‚úÖ {result['message']}")
                else:
                    failed_count = len(result.get('failed_files', []))
                    upload_stats['failed_uploads'] += failed_count
                    upload_stats['successful_uploads'] += len(result['uploaded_files'])
                    print(f"   ‚ö†Ô∏è  {result['message']}")
                    if result.get('failed_files'):
                        upload_stats['errors'].append(f"{database_name}: {failed_count} arquivos falharam")
                
            except Exception as e:
                error_msg = f"Erro no upload de {database_name}: {str(e)}"
                print(f"   ‚ùå {error_msg}")
                upload_stats['errors'].append(error_msg)
                upload_stats['failed_uploads'] += len(parquet_files)
            
            print()  # Linha em branco
        
        # Desconectar FTP
        ftp.disconnect()
        
        upload_stats['success'] = upload_stats['failed_uploads'] == 0
        return upload_stats
        
    except Exception as e:
        print(f"\n‚ùå Erro no processo de upload: {e}")
        return {
            'success': False,
            'error': str(e),
            **upload_stats
        }


def run_single_database_pipeline(
    database: str,
    output_dir: str = "data",
    verbose: bool = True,
    upload_ftp: bool = True,
    forecast_type: str = "data"
    ) -> Dict[str, Any]:
    """
    Executa pipeline de extra√ß√£o de dados SQL para um √∫nico database.
    
    Args:
        database: Nome do database para executar as queries
        output_dir: Diret√≥rio base para salvar os arquivos parquet
        verbose: Exibir logs detalhados
        upload_ftp: Fazer upload autom√°tico para FTP ap√≥s extra√ß√£o
        forecast_type: Tipo de dados para FTP (usado se upload_ftp=True)
        
    Returns:
        Dicion√°rio com estat√≠sticas de execu√ß√£o SQL e FTP (se habilitado)
    """
    print("=" * 80)
    print(f"üìä PIPELINE ETL - DATABASE: {database}")
    print("=" * 80)
    
    try:
        # Instanciar SQLQuery
        extractor = SQLQuery()
        extractor.verbose = verbose
        
        # Executar queries para o database espec√≠fico
        sql_results = extractor.execute_queries_for_database(
            database=database,
            output_dir=output_dir
        )
        
        # Verificar se houve sucesso na extra√ß√£o
        if not sql_results.get('success'):
            return {
                'success': False,
                'database': database,
                'sql_results': sql_results,
                'ftp_results': None
            }
        
        # Upload para FTP se habilitado e se houver dados extra√≠dos
        ftp_results = None
        if upload_ftp and sql_results.get('successful', 0) > 0:
            print("\n" + "=" * 80)
            print("üì§ UPLOAD PARA FTP/SFTP")
            print("=" * 80)
            
            try:
                ftp = ForecastFTPUploader()
                
                if not ftp._connect():
                    print("‚ùå Falha ao conectar no servidor FTP")
                    ftp_results = {
                        'success': False,
                        'error': 'FTP connection failed'
                    }
                else:
                    print("‚úÖ Conectado ao FTP com sucesso\n")
                    
                    # Coletar arquivos parquet do database
                    db_dir = Path(output_dir) / database
                    parquet_files = list(db_dir.glob('*.parquet'))
                    
                    if parquet_files:
                        print(f"üìÑ {len(parquet_files)} arquivos encontrados")
                        file_paths = [str(f) for f in parquet_files]
                        
                        # Upload
                        result = ftp.upload_data(
                            database_name=database,
                            forecast_type=forecast_type,
                            file_paths=file_paths
                        )
                        
                        ftp_results = {
                            'success': result['success'],
                            'uploaded_files': len(result.get('uploaded_files', [])),
                            'failed_files': len(result.get('failed_files', [])),
                            'message': result['message']
                        }
                        
                        print(f"{'‚úÖ' if result['success'] else '‚ö†Ô∏è'} {result['message']}")
                    else:
                        ftp_results = {
                            'success': False,
                            'error': 'No parquet files found'
                        }
                    
                    ftp.disconnect()
                    
            except Exception as e:
                print(f"‚ùå Erro no upload FTP: {e}")
                ftp_results = {
                    'success': False,
                    'error': str(e)
                }
        
        return {
            'success': True,
            'database': database,
            'sql_results': sql_results,
            'ftp_results': ftp_results
        }
        
    except Exception as e:
        print(f"\n‚ùå Erro no pipeline: {e}")
        return {
            'success': False,
            'database': database,
            'error': str(e),
            'sql_results': None,
            'ftp_results': None
        }


def run_single_database_supabase_pipeline(
    database: str,
    bucket_name: Optional[str] = None,
    verbose: bool = True,
    temp_dir: str = "temp"
    ) -> Dict[str, Any]:
    """
    Executa pipeline de extra√ß√£o de dados SQL para um √∫nico database e faz upload direto para Supabase.
    
    Args:
        database: Nome do database para executar as queries
        bucket_name: Nome do bucket Supabase (default: nome do database)
        verbose: Exibir logs detalhados
        temp_dir: Diret√≥rio tempor√°rio para processamento (default: "temp")
        
    Returns:
        Dicion√°rio com estat√≠sticas de execu√ß√£o SQL e Supabase
    """
    print("=" * 80)
    print(f"üìä PIPELINE ETL SUPABASE - DATABASE: {database}")
    print("=" * 80)
    
    # Determinar nome do bucket
    if bucket_name is None:
        bucket_name = database.lower().replace("_", "-")
    
    temp_path = None
    supabase_results = None
    
    try:
        # Instanciar SQLQuery
        extractor = SQLQuery()
        extractor.verbose = verbose
        
        # Executar queries para o database espec√≠fico
        sql_results = extractor.execute_queries_for_database(
            database=database,
            output_dir=temp_dir
        )
        
        # Verificar se houve sucesso na extra√ß√£o
        if not sql_results.get('success'):
            return {
                'success': False,
                'database': database,
                'bucket_name': bucket_name,
                'sql_results': sql_results,
                'supabase_results': None
            }
        
        # Verificar se h√° dados extra√≠dos
        if sql_results.get('successful', 0) == 0:
            print("‚ö†Ô∏è  Nenhum dado extra√≠do com sucesso. Pulando upload para Supabase.")
            return {
                'success': True,
                'database': database,
                'bucket_name': bucket_name,
                'sql_results': sql_results,
                'supabase_results': {
                    'success': False,
                    'error': 'No data extracted successfully'
                }
            }
        
        # Criar diret√≥rio tempor√°rio para o database
        temp_path = Path(temp_dir) / database
        
        if not temp_path.exists():
            print(f"‚ùå Diret√≥rio tempor√°rio {temp_path} n√£o encontrado ap√≥s extra√ß√£o SQL")
            return {
                'success': False,
                'database': database,
                'bucket_name': bucket_name,
                'sql_results': sql_results,
                'supabase_results': {
                    'success': False,
                    'error': 'Temporary directory not found after SQL extraction'
                }
            }
        
        # Verificar se h√° arquivos Parquet no diret√≥rio tempor√°rio
        parquet_files = list(temp_path.glob('*.parquet'))
        if not parquet_files:
            print(f"‚ö†Ô∏è  Nenhum arquivo Parquet encontrado em {temp_path}")
            return {
                'success': True,
                'database': database,
                'bucket_name': bucket_name,
                'sql_results': sql_results,
                'supabase_results': {
                    'success': False,
                    'error': 'No parquet files found in temporary directory'
                }
            }
        
        print(f"üìÑ {len(parquet_files)} arquivos Parquet encontrados para upload")
        
        # Upload para Supabase
        print("\n" + "=" * 80)
        print("üì§ UPLOAD PARA SUPABASE")
        print("=" * 80)
        
        try:
            # Inicializar SupabaseUploader
            uploader = SupabaseUploader()
            
            # Fazer upload em lote
            supabase_results = uploader.upload_directory_parquet(
                directory_path=str(temp_path),
                bucket_name=bucket_name
            )
            
            # Adicionar informa√ß√µes adicionais aos resultados
            supabase_results['success'] = supabase_results.get('failed_uploads', 0) == 0
            supabase_results['database'] = database
            supabase_results['bucket_name'] = bucket_name
            
            if supabase_results['success']:
                print(f"‚úÖ Upload para Supabase conclu√≠do com sucesso!")
                print(f"   üìÅ Bucket: {bucket_name}")
                print(f"   üìä Arquivos enviados: {supabase_results['successful_uploads']}")
            else:
                print(f"‚ö†Ô∏è  Upload para Supabase conclu√≠do com algumas falhas")
                print(f"   üìÅ Bucket: {bucket_name}")
                print(f"   ‚úÖ Sucessos: {supabase_results['successful_uploads']}")
                print(f"   ‚ùå Falhas: {supabase_results['failed_uploads']}")
                
        except Exception as e:
            print(f"‚ùå Erro no upload para Supabase: {e}")
            supabase_results = {
                'success': False,
                'error': str(e),
                'database': database,
                'bucket_name': bucket_name,
                'total_files': 0,
                'successful_uploads': 0,
                'failed_uploads': 0
            }
        
        return {
            'success': True,
            'database': database,
            'bucket_name': bucket_name,
            'sql_results': sql_results,
            'supabase_results': supabase_results
        }
        
    except Exception as e:
        print(f"\n‚ùå Erro no pipeline Supabase: {e}")
        return {
            'success': False,
            'database': database,
            'bucket_name': bucket_name,
            'error': str(e),
            'sql_results': None,
            'supabase_results': None
        }
    
    finally:
        # Limpeza: remover diret√≥rio tempor√°rio se foi criado
        if temp_path and temp_path.exists():
            try:
                shutil.rmtree(temp_path)
                if verbose:
                    print(f"üßπ Diret√≥rio tempor√°rio {temp_path} removido com sucesso")
            except Exception as e:
                if verbose:
                    print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel remover diret√≥rio tempor√°rio {temp_path}: {e}")


def print_summary(sql_results: Dict[str, Any], ftp_results: Dict[str, Any] = None):
    """
    Imprime resumo consolidado da execu√ß√£o.
    
    Args:
        sql_results: Resultados da extra√ß√£o SQL
        ftp_results: Resultados do upload FTP (opcional)
    """
    print("\n" + "=" * 80)
    print("üìã RESUMO FINAL DA EXECU√á√ÉO")
    print("=" * 80)
    
    # Resumo SQL
    print("\nüìä EXTRA√á√ÉO SQL:")
    if sql_results.get('success'):
        print(f"   ‚úÖ Sucesso: {sql_results.get('successful', 0)}/{sql_results.get('total_executions', 0)} execu√ß√µes")
        print(f"   ‚ùå Falhas: {sql_results.get('failed', 0)}/{sql_results.get('total_executions', 0)} execu√ß√µes")
        if 'total_time' in sql_results:
            print(f"   ‚è±Ô∏è  Tempo total: {sql_results['total_time']:.2f}s")
    else:
        print(f"   ‚ùå Erro: {sql_results.get('error', 'Unknown error')}")
    
    # Resumo FTP
    if ftp_results:
        print("\nüì§ UPLOAD FTP:")
        if ftp_results.get('success'):
            print(f"   ‚úÖ Sucesso: {ftp_results.get('successful_uploads', 0)}/{ftp_results.get('total_uploads', 0)} arquivos")
            print(f"   üìÅ Databases processados: {len(ftp_results.get('databases_processed', []))}")
            if ftp_results.get('databases_processed'):
                for db in ftp_results['databases_processed']:
                    print(f"      - {db}")
        else:
            print(f"   ‚ö†Ô∏è  Parcial: {ftp_results.get('successful_uploads', 0)}/{ftp_results.get('total_uploads', 0)} arquivos")
            print(f"   ‚ùå Falhas: {ftp_results.get('failed_uploads', 0)} arquivos")
            if ftp_results.get('error'):
                print(f"   ‚ùå Erro: {ftp_results['error']}")
        
        if ftp_results.get('errors'):
            print(f"\n   ‚ö†Ô∏è  Erros de upload:")
            for error in ftp_results['errors']:
                print(f"      - {error}")
    
    print("\n" + "=" * 80)
    
    # Status geral
    overall_success = (
        sql_results.get('success', False) and 
        (ftp_results is None or ftp_results.get('successful_uploads', 0) > 0)
    )
    
    if overall_success:
        print("‚úÖ Pipeline executado com sucesso!")
    else:
        print("‚ö†Ô∏è  Pipeline executado com alguns problemas")
    
    print("=" * 80 + "\n")


if __name__ == '__main__':
    # """
    # Execu√ß√£o principal do pipeline ETL:
    # 1. Extra√ß√£o de dados SQL
    # 2. Upload para FTP/SFTP
    # 3. Resumo consolidado
    
    # Para testar o pipeline Supabase, descomente as linhas abaixo:
    # """
    # start_time = time.perf_counter()
    
    # print("\nüöÄ INICIANDO PIPELINE ETL")
    # print(f"‚è∞ In√≠cio: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # # FASE 1: Extra√ß√£o de Dados SQL
    # sql_results = run_etl_pipeline(
    #     output_dir="data",
    #     verbose=True
    # )
    
    # # FASE 2: Upload para FTP (apenas se houver dados extra√≠dos com sucesso)
    # ftp_results = None
    # if sql_results.get('successful', 0) > 0:
    #     ftp_results = upload_to_ftp(
    #         data_dir="data",
    #         forecast_type="data"
    #     )
    # else:
    #     print("\n‚ö†Ô∏è  Nenhum dado extra√≠do com sucesso. Pulando upload para FTP.")
    
    # # FASE 3: Resumo Final
    # total_time = time.perf_counter() - start_time
    # print_summary(sql_results, ftp_results)
    
    # print(f"‚è±Ô∏è  Tempo total de execu√ß√£o: {total_time:.2f}s")
    # print(f"‚è∞ T√©rmino: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # EXEMPLO DE USO DO PIPELINE SUPABASE (descomente para testar)
    print("\n" + "="*80)
    print("üß™ TESTE DO PIPELINE SUPABASE")
    print("="*80)
    
    # Testar pipeline Supabase para um database espec√≠fico
    supabase_result = run_single_database_supabase_pipeline(
        database="013BW_ERP_BI",
        bucket_name="013bw-erp-bi",
        verbose=True,
        temp_dir="temp"
    )
    
    print("\nüìä RESULTADO PIPELINE SUPABASE:")
    print(f"   ‚úÖ Sucesso: {supabase_result['success']}")
    print(f"   üìÅ Database: {supabase_result['database']}")
    print(f"   ü™£ Bucket: {supabase_result['bucket_name']}")
    
    if supabase_result['supabase_results']:
        sb_results = supabase_result['supabase_results']
        print(f"   üìä Arquivos enviados: {sb_results.get('successful_uploads', 0)}")
        print(f"   ‚ùå Falhas: {sb_results.get('failed_uploads', 0)}")
    
    print("="*80)

